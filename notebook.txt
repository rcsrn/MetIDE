# %% [markdown]
# # üìö Fase 1: Procesamiento de Documentos MapStruct para Sistema RAG
# 
# Este notebook procesa documentaci√≥n de MapStruct, genera embeddings vectoriales y los almacena en Supabase para su uso posterior en un sistema RAG.
# 
# ## üìã Requisitos Previos:
# 1. Cuenta gratuita en Supabase con proyecto creado
# 2. PDF de documentaci√≥n MapStruct
# 3. Archivos TXT con reglas internas (opcional)

# %% [markdown]
# ## 1Ô∏è‚É£ Configuraci√≥n Inicial y Gesti√≥n de Secretos

# %%
# Celda 1: Instalaci√≥n de dependencias necesarias
print("üîß Instalando dependencias...")
!pip install -q sentence-transformers==2.2.2
!pip install -q langchain==0.1.0
!pip install -q langchain-community
!pip install -q pypdf2==3.0.1
!pip install -q supabase==2.3.0
!pip install -q tiktoken==0.5.2
!pip install -q tqdm
!pip install -q numpy

print("‚úÖ Dependencias instaladas correctamente")

# %%
# Celda 2: Importar librer√≠as
import os
import json
import hashlib
from typing import List, Dict, Tuple
import numpy as np
from tqdm import tqdm
from datetime import datetime
from google.colab import userdata, files
import warnings
warnings.filterwarnings('ignore')

# Librer√≠as para procesamiento
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

# Supabase
from supabase import create_client, Client

print("‚úÖ Librer√≠as importadas correctamente")

# %% [markdown]
# ## 2Ô∏è‚É£ Configuraci√≥n de Secretos de Google Colab
# 
# Para configurar tus secretos:
# 1. Click en el √≠cono de üîë (llave) en el panel izquierdo
# 2. Agregar los siguientes secretos:
#    - `SUPABASE_URL`: Tu URL de Supabase
#    - `SUPABASE_KEY`: Tu clave an√≥nima de Supabase

# %%
# Celda 3: Cargar secretos de forma segura
try:
    # Obtener secretos desde Google Colab
    SUPABASE_URL = userdata.get('SUPABASE_URL')
    SUPABASE_KEY = userdata.get('SUPABASE_KEY')
    
    # Validar que los secretos existen
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise ValueError("Secretos no configurados")
    
    print("‚úÖ Secretos cargados correctamente")
    print(f"üìç Supabase URL: {SUPABASE_URL[:20]}...")
    
except Exception as e:
    print("‚ùå Error: No se encontraron los secretos de Colab")
    print("Por favor, configura los siguientes secretos en Colab:")
    print("1. Click en el √≠cono üîë en el panel izquierdo")
    print("2. Agrega SUPABASE_URL y SUPABASE_KEY")
    raise e

# %% [markdown]
# ## 3Ô∏è‚É£ Configuraci√≥n de la Base de Datos en Supabase
# 
# Ejecuta este SQL en tu dashboard de Supabase antes de continuar:

# %%
# Celda 4: Mostrar SQL necesario para Supabase
sql_setup = """
-- Crear extensi√≥n para vectores
CREATE EXTENSION IF NOT EXISTS vector;

-- Crear tabla para documentos
CREATE TABLE IF NOT EXISTS mapstruct_documents (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(384),
    metadata JSONB,
    chunk_hash VARCHAR(64) UNIQUE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Crear √≠ndice para b√∫squeda vectorial eficiente
CREATE INDEX IF NOT EXISTS idx_mapstruct_embedding 
ON mapstruct_documents 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Funci√≥n para b√∫squeda de similitud
CREATE OR REPLACE FUNCTION match_mapstruct_documents (
    query_embedding vector(384),
    match_threshold float,
    match_count int
)
RETURNS TABLE (
    id int,
    content text,
    metadata jsonb,
    similarity float
)
LANGUAGE sql STABLE
AS $$
    SELECT 
        id,
        content,
        metadata,
        1 - (embedding <=> query_embedding) as similarity
    FROM mapstruct_documents
    WHERE 1 - (embedding <=> query_embedding) > match_threshold
    ORDER BY embedding <=> query_embedding
    LIMIT match_count;
$$;
"""

print("üìã SQL para ejecutar en Supabase Dashboard:")
print("=" * 50)
print(sql_setup)
print("=" * 50)
print("\n‚ö†Ô∏è Aseg√∫rate de ejecutar este SQL en tu dashboard de Supabase antes de continuar")

# %% [markdown]
# ## 4Ô∏è‚É£ Carga de Documentos PDF

# %%
# Celda 5: Funci√≥n para cargar archivos
def upload_documents():
    """Permite cargar archivos PDF y TXT desde tu computadora"""
    print("üì§ Por favor, selecciona tus archivos PDF y/o TXT")
    uploaded = files.upload()
    
    documents = []
    for filename, content in uploaded.items():
        doc_info = {
            'filename': filename,
            'content': content,
            'type': 'pdf' if filename.endswith('.pdf') else 'txt'
        }
        documents.append(doc_info)
        print(f"‚úÖ Archivo cargado: {filename} ({len(content):,} bytes)")
    
    return documents

# Cargar documentos
print("üîÑ Iniciando carga de documentos...")
uploaded_docs = upload_documents()

# %% [markdown]
# ## 5Ô∏è‚É£ Procesamiento de Documentos

# %%
# Celda 6: Funciones de procesamiento de documentos
class DocumentProcessor:
    """Procesador optimizado para documentos t√©cnicos"""
    
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=[
                "\n\n",      # P√°rrafos
                "\n",        # L√≠neas
                ".",         # Oraciones
                ";",         # Punto y coma
                ",",         # Comas
                " ",         # Espacios
                ""           # Caracteres individuales
            ]
        )
    
    def process_pdf(self, content: bytes, filename: str) -> str:
        """Extrae texto de un PDF con manejo de errores mejorado"""
        try:
            # Guardar temporalmente el archivo
            temp_path = f"/tmp/{filename}"
            with open(temp_path, 'wb') as f:
                f.write(content)
            
            # Leer el PDF
            text = ""
            with open(temp_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                print(f"üìñ Procesando {total_pages} p√°ginas de {filename}")
                
                for page_num in tqdm(range(total_pages), desc="P√°ginas"):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    
                    # Limpiar el texto
                    page_text = self.clean_text(page_text)
                    text += page_text + "\n\n"
            
            # Eliminar archivo temporal
            os.remove(temp_path)
            return text
            
        except Exception as e:
            print(f"‚ùå Error procesando PDF {filename}: {str(e)}")
            return ""
    
    def process_txt(self, content: bytes, filename: str) -> str:
        """Procesa archivo de texto plano"""
        try:
            text = content.decode('utf-8')
            return self.clean_text(text)
        except Exception as e:
            print(f"‚ùå Error procesando TXT {filename}: {str(e)}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """Limpia y normaliza el texto"""
        # Eliminar espacios m√∫ltiples
        text = ' '.join(text.split())
        
        # Eliminar caracteres especiales problem√°ticos
        text = text.replace('\x00', '')
        text = text.replace('\r\n', '\n')
        
        # Mantener estructura de p√°rrafos
        text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())
        
        return text
    
    def create_chunks(self, text: str, metadata: Dict) -> List[Dict]:
        """Crea chunks con metadatos enriquecidos"""
        chunks = self.text_splitter.split_text(text)
        
        chunk_docs = []
        for i, chunk in enumerate(chunks):
            # Generar hash √∫nico para evitar duplicados
            chunk_hash = hashlib.sha256(chunk.encode()).hexdigest()
            
            chunk_doc = {
                'content': chunk,
                'metadata': {
                    **metadata,
                    'chunk_index': i,
                    'chunk_size': len(chunk),
                    'total_chunks': len(chunks)
                },
                'chunk_hash': chunk_hash
            }
            chunk_docs.append(chunk_doc)
        
        return chunk_docs

# Inicializar procesador
processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)
print("‚úÖ Procesador de documentos inicializado")

# %%
# Celda 7: Procesar todos los documentos cargados
all_chunks = []

for doc in uploaded_docs:
    print(f"\nüîÑ Procesando: {doc['filename']}")
    
    # Extraer texto seg√∫n el tipo de archivo
    if doc['type'] == 'pdf':
        text = processor.process_pdf(doc['content'], doc['filename'])
    else:
        text = processor.process_txt(doc['content'], doc['filename'])
    
    if text:
        # Crear metadatos base
        metadata = {
            'source': doc['filename'],
            'type': doc['type'],
            'processed_at': datetime.now().isoformat()
        }
        
        # Crear chunks
        chunks = processor.create_chunks(text, metadata)
        all_chunks.extend(chunks)
        
        print(f"‚úÖ Generados {len(chunks)} chunks de {doc['filename']}")
        print(f"   Caracteres totales: {len(text):,}")
    else:
        print(f"‚ö†Ô∏è No se pudo extraer texto de {doc['filename']}")

print(f"\nüìä Resumen:")
print(f"Total de chunks generados: {len(all_chunks)}")
print(f"Tama√±o promedio de chunk: {np.mean([len(c['content']) for c in all_chunks]):.0f} caracteres")

# %% [markdown]
# ## 6Ô∏è‚É£ Generaci√≥n de Embeddings con Sentence Transformers

# %%
# Celda 8: Configurar y cargar modelo de embeddings
print("ü§ñ Cargando modelo de embeddings...")
print("Modelo: all-MiniLM-L6-v2")
print("Dimensiones: 384")
print("Optimizado para: B√∫squeda sem√°ntica")

# Cargar modelo
model = SentenceTransformer('all-MiniLM-L6-v2')

# Informaci√≥n del modelo
print("\nüìä Informaci√≥n del modelo:")
print(f"Max sequence length: {model.max_seq_length}")
print(f"Embedding dimension: {model.get_sentence_embedding_dimension()}")
print("‚úÖ Modelo cargado correctamente")

# %%
# Celda 9: Generar embeddings para todos los chunks
def generate_embeddings_batch(chunks: List[Dict], model, batch_size: int = 32) -> List[Dict]:
    """Genera embeddings en lotes para mejor rendimiento"""
    
    # Preparar textos para embedding
    texts = [chunk['content'] for chunk in chunks]
    
    # Generar embeddings en lotes
    print(f"üîÑ Generando embeddings para {len(texts)} chunks...")
    embeddings = []
    
    for i in tqdm(range(0, len(texts), batch_size), desc="Lotes"):
        batch_texts = texts[i:i + batch_size]
        batch_embeddings = model.encode(
            batch_texts,
            normalize_embeddings=True,  # Normalizar para cosine similarity
            show_progress_bar=False
        )
        embeddings.extend(batch_embeddings)
    
    # Agregar embeddings a los chunks
    for chunk, embedding in zip(chunks, embeddings):
        chunk['embedding'] = embedding.tolist()
    
    return chunks

# Generar embeddings
chunks_with_embeddings = generate_embeddings_batch(all_chunks, model, batch_size=32)

print(f"‚úÖ Embeddings generados para {len(chunks_with_embeddings)} chunks")
print(f"üìê Dimensi√≥n de embeddings: {len(chunks_with_embeddings[0]['embedding'])}")

# %% [markdown]
# ## 7Ô∏è‚É£ Almacenamiento en Supabase

# %%
# Celda 10: Conectar con Supabase y crear funciones de almacenamiento
class SupabaseVectorStore:
    """Gestiona el almacenamiento de vectores en Supabase"""
    
    def __init__(self, url: str, key: str):
        self.client = create_client(url, key)
        print("‚úÖ Conectado a Supabase")
    
    def insert_documents(self, documents: List[Dict], batch_size: int = 50):
        """Inserta documentos en lotes con manejo de duplicados"""
        
        total = len(documents)
        inserted = 0
        duplicates = 0
        errors = 0
        
        print(f"üîÑ Insertando {total} documentos en Supabase...")
        
        for i in tqdm(range(0, total, batch_size), desc="Insertando"):
            batch = documents[i:i + batch_size]
            
            for doc in batch:
                try:
                    # Preparar documento para inserci√≥n
                    db_doc = {
                        'content': doc['content'],
                        'embedding': doc['embedding'],
                        'metadata': doc['metadata'],
                        'chunk_hash': doc['chunk_hash']
                    }
                    
                    # Intentar insertar
                    response = self.client.table('mapstruct_documents').insert(db_doc).execute()
                    inserted += 1
                    
                except Exception as e:
                    if 'duplicate key' in str(e).lower():
                        duplicates += 1
                    else:
                        errors += 1
                        print(f"‚ùå Error: {str(e)[:100]}")
        
        print(f"\nüìä Resultados de inserci√≥n:")
        print(f"‚úÖ Insertados: {inserted}")
        print(f"‚ö†Ô∏è Duplicados omitidos: {duplicates}")
        print(f"‚ùå Errores: {errors}")
        
        return inserted
    
    def verify_storage(self):
        """Verifica que los documentos se almacenaron correctamente"""
        try:
            response = self.client.table('mapstruct_documents').select('count').execute()
            count = response.count
            print(f"‚úÖ Total de documentos en la base de datos: {count}")
            return count
        except Exception as e:
            print(f"‚ùå Error verificando almacenamiento: {str(e)}")
            return 0
    
    def test_similarity_search(self, query: str, model):
        """Prueba la b√∫squeda por similitud"""
        print(f"\nüîç Probando b√∫squeda: '{query}'")
        
        # Generar embedding para la consulta
        query_embedding = model.encode(query, normalize_embeddings=True).tolist()
        
        # Buscar documentos similares
        response = self.client.rpc('match_mapstruct_documents', {
            'query_embedding': query_embedding,
            'match_threshold': 0.5,
            'match_count': 3
        }).execute()
        
        if response.data:
            print(f"‚úÖ Encontrados {len(response.data)} resultados relevantes:")
            for i, result in enumerate(response.data, 1):
                print(f"\n{i}. Similitud: {result['similarity']:.3f}")
                print(f"   Contenido: {result['content'][:200]}...")
        else:
            print("‚ùå No se encontraron resultados")
        
        return response.data

# Inicializar store
vector_store = SupabaseVectorStore(SUPABASE_URL, SUPABASE_KEY)

# %%
# Celda 11: Almacenar documentos en Supabase
# Insertar documentos
inserted_count = vector_store.insert_documents(chunks_with_embeddings, batch_size=50)

# Verificar almacenamiento
total_in_db = vector_store.verify_storage()

# %% [markdown]
# ## 8Ô∏è‚É£ Pruebas de B√∫squeda Vectorial

# %%
# Celda 12: Realizar pruebas de b√∫squeda
test_queries = [
    "How to map a list of DTOs in MapStruct?",
    "What are MapStruct annotations?",
    "Custom mapping methods",
    "Null value handling in MapStruct"
]

print("üß™ Ejecutando pruebas de b√∫squeda...")
print("=" * 50)

for query in test_queries:
    results = vector_store.test_similarity_search(query, model)
    print("-" * 50)

# %% [markdown]
# ## 9Ô∏è‚É£ Estad√≠sticas y Optimizaci√≥n

# %%
# Celda 13: Generar estad√≠sticas del proceso
def generate_statistics(chunks: List[Dict]):
    """Genera estad√≠sticas √∫tiles sobre el procesamiento"""
    
    stats = {
        'total_chunks': len(chunks),
        'total_characters': sum(len(c['content']) for c in chunks),
        'avg_chunk_size': np.mean([len(c['content']) for c in chunks]),
        'min_chunk_size': min(len(c['content']) for c in chunks),
        'max_chunk_size': max(len(c['content']) for c in chunks),
        'unique_sources': len(set(c['metadata']['source'] for c in chunks))
    }
    
    print("üìä Estad√≠sticas del Procesamiento:")
    print("=" * 50)
    print(f"üìÑ Total de chunks: {stats['total_chunks']:,}")
    print(f"üìù Total de caracteres: {stats['total_characters']:,}")
    print(f"üìê Tama√±o promedio de chunk: {stats['avg_chunk_size']:.0f} chars")
    print(f"üìè Rango de tama√±os: {stats['min_chunk_size']} - {stats['max_chunk_size']} chars")
    print(f"üìö Fuentes √∫nicas procesadas: {stats['unique_sources']}")
    
    # Distribuci√≥n por fuente
    print("\nüìë Distribuci√≥n por fuente:")
    sources = {}
    for chunk in chunks:
        source = chunk['metadata']['source']
        sources[source] = sources.get(source, 0) + 1
    
    for source, count in sources.items():
        percentage = (count / stats['total_chunks']) * 100
        print(f"  ‚Ä¢ {source}: {count} chunks ({percentage:.1f}%)")
    
    return stats

# Generar estad√≠sticas
stats = generate_statistics(chunks_with_embeddings)

# %% [markdown]
# ## üéØ Resumen y Pr√≥ximos Pasos

# %%
# Celda 14: Resumen final y exportar configuraci√≥n
config_summary = {
    'supabase_url': SUPABASE_URL,
    'table_name': 'mapstruct_documents',
    'embedding_model': 'all-MiniLM-L6-v2',
    'embedding_dimensions': 384,
    'chunk_size': processor.chunk_size,
    'chunk_overlap': processor.chunk_overlap,
    'total_documents_processed': len(uploaded_docs),
    'total_chunks_created': stats['total_chunks'],
    'total_chunks_stored': total_in_db,
    'processing_date': datetime.now().isoformat()
}

print("‚úÖ PROCESO COMPLETADO EXITOSAMENTE")
print("=" * 50)
print("\nüìã Configuraci√≥n para usar en el backend Lambda:")
print(json.dumps(config_summary, indent=2))

print("\nüöÄ Pr√≥ximos pasos:")
print("1. ‚úÖ Base de conocimientos creada en Supabase")
print("2. ‚è≠Ô∏è Configurar funci√≥n AWS Lambda con estos par√°metros")
print("3. ‚è≠Ô∏è Desarrollar el plugin de VS Code")
print("4. ‚è≠Ô∏è Realizar pruebas de integraci√≥n")

# Guardar configuraci√≥n como archivo JSON
config_filename = 'rag_config.json'
with open(config_filename, 'w') as f:
    json.dump(config_summary, f, indent=2)

# Descargar archivo de configuraci√≥n
files.download(config_filename)
print(f"\nüíæ Archivo de configuraci√≥n '{config_filename}' descargado")

# %% [markdown]
# ## üîß Funciones √ötiles para Mantenimiento

# %%
# Celda 15: Funciones auxiliares para mantenimiento futuro
def update_knowledge_base(new_pdf_path: str):
    """Funci√≥n para actualizar la base de conocimientos con nuevos documentos"""
    print(f"üì§ Actualizando base de conocimientos con: {new_pdf_path}")
    # Implementar l√≥gica de actualizaci√≥n incremental
    pass

def clear_database():
    """Limpia toda la base de datos (usar con precauci√≥n)"""
    confirm = input("‚ö†Ô∏è ¬øEst√°s seguro de que quieres limpiar toda la base de datos? (yes/no): ")
    if confirm.lower() == 'yes':
        try:
            response = vector_store.client.table('mapstruct_documents').delete().neq('id', 0).execute()
            print("‚úÖ Base de datos limpiada")
        except Exception as e:
            print(f"‚ùå Error: {e}")
    else:
        print("‚ùå Operaci√≥n cancelada")

def export_chunks_to_json():
    """Exporta todos los chunks a un archivo JSON para backup"""
    filename = f'chunks_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    
    # Remover embeddings para reducir tama√±o
    chunks_no_embeddings = []
    for chunk in chunks_with_embeddings:
        chunk_copy = chunk.copy()
        chunk_copy.pop('embedding', None)
        chunks_no_embeddings.append(chunk_copy)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(chunks_no_embeddings, f, ensure_ascii=False, indent=2)
    
    files.download(filename)
    print(f"‚úÖ Backup exportado: {filename}")

print("üîß Funciones de mantenimiento disponibles:")
print("  ‚Ä¢ update_knowledge_base() - Actualizar con nuevos documentos")
print("  ‚Ä¢ clear_database() - Limpiar base de datos")
print("  ‚Ä¢ export_chunks_to_json() - Exportar backup de chunks")

# %% [markdown]
# ---
# ## üìö Documentaci√≥n Adicional
# 
# ### Modelo de Embeddings Utilizado
# - **Modelo**: `all-MiniLM-L6-v2`
# - **Dimensiones**: 384
# - **Ventajas**: R√°pido, eficiente, excelente para b√∫squeda sem√°ntica
# - **Tama√±o**: ~80MB
# - **Velocidad**: ~14,200 oraciones/segundo en GPU
# 
# ### Estrategia de Chunking
# - **Tama√±o**: 1000 caracteres (√≥ptimo para contexto)
# - **Overlap**: 200 caracteres (preserva contexto entre chunks)
# - **Separadores**: Jer√°rquicos (p√°rrafos > oraciones > palabras)
# 
# ### Optimizaciones Implementadas
# 1. **Deduplicaci√≥n**: Hash SHA-256 previene duplicados
# 2. **Batch Processing**: Mejora velocidad 10x
# 3. **Normalizaci√≥n**: Embeddings normalizados para cosine similarity
# 4. **Metadatos enriquecidos**: Facilita filtrado y debugging
# 
# ### Troubleshooting Com√∫n
# 
# | Problema | Soluci√≥n |
# |----------|----------|
# | Error de conexi√≥n a Supabase | Verificar URL y KEY en secretos |
# | Chunks muy grandes/peque√±os | Ajustar chunk_size en DocumentProcessor |
# | B√∫squedas sin resultados | Reducir match_threshold (0.5 ‚Üí 0.3) |
# | Duplicados en inserci√≥n | Normal, el sistema los maneja autom√°ticamente |

print("‚úÖ Notebook completado. ¬°Tu base de conocimientos RAG est√° lista!")